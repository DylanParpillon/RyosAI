# =============================================================================
# CORE/LLM.PY - Connexion au cerveau IA (Groq)
# =============================================================================
# Ce fichier g√®re la communication avec Groq, le service qui fournit l'IA.
#
# Comment √ßa marche:
# 1. On envoie un "prompt syst√®me" (la personnalit√© de Ryosa)
# 2. On envoie l'historique de la conversation
# 3. Groq g√©n√®re une r√©ponse intelligente
# 4. On retourne cette r√©ponse
#
# NOTE: Ce fichier est pr√©vu pour √™tre migr√© vers Ollama quand tu auras
#       install√© ta VM. Pour l'instant, on utilise Groq (cloud).
# =============================================================================

from groq import Groq
from typing import List, Dict, Optional
import logging

# Configuration du logging
logger = logging.getLogger("ryosa.llm")


class ClientIA:
    """
    Client pour communiquer avec Groq (le cerveau de Ryosa).
    
    Exemple d'utilisation:
        client = ClientIA(cle_api="ta_cl√©_api")
        reponse = client.generer_reponse(
            prompt_systeme="Tu es Ryosa...",
            messages=[{"role": "user", "content": "Salut!"}]
        )
    """
    
    def __init__(
        self,
        cle_api: str,
        modele: str = "llama-3.1-8b-instant"
    ):
        """
        Initialise le client Groq.
        
        Args:
            cle_api: Ta cl√© API Groq (depuis console.groq.com)
            modele: Le mod√®le √† utiliser (par d√©faut: llama-3.1-8b-instant)
                   Autres options: llama-3.3-70b-versatile, mixtral-8x7b-32768
        """
        self.client = Groq(api_key=cle_api)
        self.modele = modele
        
        # Param√®tres de g√©n√©ration
        self.creativite = 0.7      # 0 = tr√®s pr√©visible, 1 = tr√®s cr√©atif
        self.longueur_max = 150    # Limite la longueur des r√©ponses
        
        logger.info(f"ClientIA initialis√© avec le mod√®le: {modele}")
    
    def generer_reponse(
        self,
        prompt_systeme: str,
        messages: List[Dict[str, str]],
        creativite: Optional[float] = None
    ) -> str:
        """
        G√©n√®re une r√©ponse de Ryosa.
        
        Args:
            prompt_systeme: La personnalit√© de Ryosa (instructions pour l'IA)
            messages: L'historique de conversation sous forme de liste
                      [{"role": "user", "content": "Salut!"}]
            creativite: Cr√©ativit√© (optionnel, utilise la valeur par d√©faut sinon)
        
        Returns:
            La r√©ponse g√©n√©r√©e par Ryosa
        
        Exemple:
            messages = [
                {"role": "user", "content": "Salut Ryosa!"},
                {"role": "assistant", "content": "Coucou! Comment √ßa va?"},
                {"role": "user", "content": "√áa va bien, et toi?"}
            ]
            reponse = client.generer_reponse(prompt_systeme, messages)
        """
        try:
            # On construit la requ√™te pour Groq
            tous_les_messages = [
                {"role": "system", "content": prompt_systeme}
            ] + messages
            
            # On appelle l'API Groq
            reponse = self.client.chat.completions.create(
                model=self.modele,
                messages=tous_les_messages,
                temperature=creativite or self.creativite,
                max_tokens=self.longueur_max,
            )
            
            # On extrait le texte de la r√©ponse
            texte_genere = reponse.choices[0].message.content
            
            logger.debug(f"R√©ponse g√©n√©r√©e: {texte_genere[:50]}...")
            return texte_genere.strip()
            
        except Exception as erreur:
            # En cas d'erreur, on log et on retourne un message par d√©faut
            logger.error(f"Erreur lors de la g√©n√©ration: {erreur}")
            return self._obtenir_reponse_secours()
    
    def _obtenir_reponse_secours(self) -> str:
        """
        R√©ponse de secours si Groq ne fonctionne pas.
        
        C'est important d'avoir un fallback pour que Ryosa puisse
        toujours r√©pondre quelque chose m√™me en cas de probl√®me.
        """
        reponses_secours = [
            "Hmm, j'ai un petit bug l√†... R√©essaie dans un moment! üí´",
            "Oups, mon cerveau fait une pause! Attends un peu~ üåô",
            "Ah, je r√©fl√©chis trop fort l√†! Redemande-moi √ßa? ‚ú®",
        ]
        import random
        return random.choice(reponses_secours)
    
    def definir_creativite(self, niveau: str):
        """
        Ajuste la cr√©ativit√© de Ryosa.
        
        Args:
            niveau: "bas" (pr√©visible), "moyen" (√©quilibr√©), "haut" (cr√©atif)
        """
        niveaux = {
            "bas": 0.3,
            "moyen": 0.7,
            "haut": 0.9
        }
        self.creativite = niveaux.get(niveau, 0.7)
        logger.info(f"Cr√©ativit√© ajust√©e: {niveau} (temp={self.creativite})")


# =============================================================================
# TEST DU CLIENT IA
# =============================================================================
if __name__ == "__main__":
    # Ce code s'ex√©cute uniquement si tu lances ce fichier directement
    # python core/llm.py
    
    import os
    from dotenv import load_dotenv
    
    load_dotenv()
    
    cle_api = os.getenv("GROQ_API_KEY")
    
    if not cle_api:
        print("‚ùå GROQ_API_KEY non trouv√©e dans .env!")
        print("   Va sur https://console.groq.com pour en obtenir une")
        exit(1)
    
    print("üß† Test du client IA (Groq)")
    print("=" * 50)
    
    client = ClientIA(cle_api=cle_api)
    
    # Test simple
    prompt_systeme = "Tu es Ryosa, une IA mignonne et serviable. R√©ponds en fran√ßais."
    messages = [
        {"role": "user", "content": "Salut Ryosa! Comment tu vas?"}
    ]
    
    print("\nüì§ Envoi du message de test...")
    reponse = client.generer_reponse(prompt_systeme, messages)
    print(f"\nüì• R√©ponse de Ryosa:\n   {reponse}")
