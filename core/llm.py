# =============================================================================
# CORE/LLM.PY - Connexion au cerveau IA (Ollama)
# =============================================================================
# Ce fichier g√®re la communication avec Ollama, le service d'IA locale.
#
# Comment √ßa marche:
# 1. On envoie un "prompt syst√®me" (la personnalit√© de Ryosa)
# 2. On envoie l'historique de la conversation
# 3. Ollama g√©n√®re une r√©ponse intelligente
# 4. On retourne cette r√©ponse
# =============================================================================

import ollama
from typing import List, Dict, Optional
import logging

# Configuration du logging
logger = logging.getLogger("ryosa.llm")


class ClientIA:
    """
    Client pour communiquer avec Ollama (le cerveau de Ryosa).
    
    Exemple d'utilisation:
        client = ClientIA()
        reponse = client.generer_reponse(
            prompt_systeme="Tu es Ryosa...",
            messages=[{"role": "user", "content": "Salut!"}]
        )
    """
    
    def __init__(
        self,
        url_ollama: str = "http://localhost:11434",
        modele: str = "llama3.1"
    ):
        """
        Initialise le client Ollama.
        
        Args:
            url_ollama: URL du serveur Ollama (par d√©faut: localhost:11434)
            modele: Le mod√®le √† utiliser (par d√©faut: llama3.1)
                   Autres options: mistral, qwen2, phi3, etc.
        """
        self.url_ollama = url_ollama
        self.modele = modele
        
        # Param√®tres de g√©n√©ration
        self.creativite = 0.7      # 0 = tr√®s pr√©visible, 1 = tr√®s cr√©atif
        self.longueur_max = 150    # Limite la longueur des r√©ponses
        
        # Configurer le client Ollama
        self.client = ollama.Client(host=url_ollama)
        
        logger.info(f"ClientIA initialis√© - Mod√®le: {modele}, URL: {url_ollama}")
    
    def generer_reponse(
        self,
        prompt_systeme: str,
        messages: List[Dict[str, str]],
        creativite: Optional[float] = None
    ) -> str:
        """
        G√©n√®re une r√©ponse de Ryosa.
        
        Args:
            prompt_systeme: La personnalit√© de Ryosa (instructions pour l'IA)
            messages: L'historique de conversation sous forme de liste
                      [{"role": "user", "content": "Salut!"}]
            creativite: Cr√©ativit√© (optionnel, utilise la valeur par d√©faut sinon)
        
        Returns:
            La r√©ponse g√©n√©r√©e par Ryosa
        
        Exemple:
            messages = [
                {"role": "user", "content": "Salut Ryosa!"},
                {"role": "assistant", "content": "Coucou! Comment √ßa va?"},
                {"role": "user", "content": "√áa va bien, et toi?"}
            ]
            reponse = client.generer_reponse(prompt_systeme, messages)
        """
        try:
            # On construit la requ√™te pour Ollama
            tous_les_messages = [
                {"role": "system", "content": prompt_systeme}
            ] + messages
            
            # On appelle l'API Ollama
            reponse = self.client.chat(
                model=self.modele,
                messages=tous_les_messages,
                options={
                    "temperature": creativite or self.creativite,
                    "num_predict": self.longueur_max,
                }
            )
            
            # On extrait le texte de la r√©ponse
            texte_genere = reponse["message"]["content"]
            
            logger.debug(f"R√©ponse g√©n√©r√©e: {texte_genere[:50]}...")
            return texte_genere.strip()
            
        except Exception as erreur:
            # En cas d'erreur, on log et on retourne un message par d√©faut
            logger.error(f"Erreur lors de la g√©n√©ration: {erreur}")
            return self._obtenir_reponse_secours()
    
    def _obtenir_reponse_secours(self) -> str:
        """
        R√©ponse de secours si Ollama ne fonctionne pas.
        
        C'est important d'avoir un fallback pour que Ryosa puisse
        toujours r√©pondre quelque chose m√™me en cas de probl√®me.
        """
        reponses_secours = [
            "Hmm, j'ai un petit bug l√†... R√©essaie dans un moment! üí´",
            "Oups, mon cerveau fait une pause! Attends un peu~ üåô",
            "Ah, je r√©fl√©chis trop fort l√†! Redemande-moi √ßa? ‚ú®",
        ]
        import random
        return random.choice(reponses_secours)
    
    def definir_creativite(self, niveau: str):
        """
        Ajuste la cr√©ativit√© de Ryosa.
        
        Args:
            niveau: "bas" (pr√©visible), "moyen" (√©quilibr√©), "haut" (cr√©atif)
        """
        niveaux = {
            "bas": 0.3,
            "moyen": 0.7,
            "haut": 0.9
        }
        self.creativite = niveaux.get(niveau, 0.7)
        logger.info(f"Cr√©ativit√© ajust√©e: {niveau} (temp={self.creativite})")
    
    def verifier_connexion(self) -> bool:
        """
        V√©rifie que la connexion √† Ollama fonctionne.
        
        Returns:
            True si Ollama est accessible, False sinon
        """
        try:
            # Teste la connexion en listant les mod√®les
            modeles = self.client.list()
            logger.info(f"Connexion Ollama OK - {len(modeles.get('models', []))} mod√®les disponibles")
            return True
        except Exception as erreur:
            logger.error(f"Impossible de se connecter √† Ollama: {erreur}")
            return False


# =============================================================================
# TEST DU CLIENT IA
# =============================================================================
if __name__ == "__main__":
    # Ce code s'ex√©cute uniquement si tu lances ce fichier directement
    # python core/llm.py
    
    print("üß† Test du client IA (Ollama)")
    print("=" * 50)
    
    client = ClientIA()
    
    # V√©rifier la connexion
    print("\nüîå V√©rification de la connexion Ollama...")
    if not client.verifier_connexion():
        print("‚ùå Impossible de se connecter √† Ollama!")
        print("   Assure-toi qu'Ollama est lanc√©: ollama serve")
        exit(1)
    
    print("‚úÖ Connexion OK!")
    
    # Test simple
    prompt_systeme = "Tu es Ryosa, une IA mignonne et serviable. R√©ponds en fran√ßais."
    messages = [
        {"role": "user", "content": "Salut Ryosa! Comment tu vas?"}
    ]
    
    print("\nüì§ Envoi du message de test...")
    reponse = client.generer_reponse(prompt_systeme, messages)
    print(f"\nüì• R√©ponse de Ryosa:\n   {reponse}")
